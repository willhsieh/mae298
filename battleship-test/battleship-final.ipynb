{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Battleship Model Comparison\n",
    "\n",
    "This notebook uses the refactored Python helpers in `battleship-test/battleship`\n",
    "to compare multiple Bayesian search strategies on the same boards.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "repo_root = Path.cwd()\n",
    "if (repo_root / \"battleship-test\" / \"battleship\").exists():\n",
    "    sys.path.insert(0, str(repo_root / \"battleship-test\"))\n",
    "elif (repo_root / \"battleship\").exists():\n",
    "    sys.path.insert(0, str(repo_root))\n",
    "\n",
    "from battleship.config import BenchmarkConfig, GameConfig, MonteCarloConfig, SearchConfig\n",
    "from battleship.board import generate_board\n",
    "from battleship.evaluate import benchmark_algorithms\n",
    "from battleship.plotting import plot_board\n",
    "from battleship.priors import center_bias_prior, edge_bias_prior, uniform_prior\n",
    "from battleship.search import ALGORITHMS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_config = GameConfig()\n",
    "search_config = SearchConfig()\n",
    "mc_config = MonteCarloConfig()\n",
    "rng = np.random.default_rng(298)\n",
    "\n",
    "board = generate_board(rng, game_config)\n",
    "plot_board(board)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = uniform_prior(game_config.board_size)\n",
    "algorithms = [\"adjacent\", \"orientation\", \"placement\", \"hunt_target\", \"monte_carlo\"]\n",
    "\n",
    "for offset, name in enumerate(algorithms, start=1):\n",
    "    algo_rng = np.random.default_rng(298 + offset)\n",
    "    result = ALGORITHMS[name](\n",
    "        board, prior, algo_rng, game_config, search_config, mc_config\n",
    "    )\n",
    "    print(f\"{name}: {result.shots} shots\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_config = BenchmarkConfig(boards=50, seed=298)\n",
    "prior_name, prior_fn = \"uniform\", uniform_prior\n",
    "\n",
    "results = benchmark_algorithms(\n",
    "    algorithms, prior_fn, benchmark_config, game_config, search_config, mc_config\n",
    ")\n",
    "\n",
    "print(f\"Benchmark results (prior={prior_name})\")\n",
    "for name in algorithms:\n",
    "    result = results[name]\n",
    "    print(\n",
    "        f\"{name:12s} mean={result.mean:6.2f} std={result.std:6.2f} \"\n",
    "        f\"median={result.median:6.2f} min={result.min_shots:3d} max={result.max_shots:3d}\"\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}